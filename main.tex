\documentclass[12pt,a4paper]{report}

% --- Pacotes essenciais ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[brazil]{babel}
\usepackage{geometry}
\usepackage{graphicx}   % Essencial para inserir imagens
\usepackage{array}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{float}
\usepackage{amsmath}
\usepackage{seqsplit}
\usepackage[hidelinks]{hyperref} % hidelinks remove as caixas coloridas dos links
\usepackage{mathptmx}   % Fonte Times New Roman no documento todo
\usepackage{setspace}   % Para espaçamento entre linhas
\usepackage{titlesec}   % PACOTE NOVO: Para controlar o tamanho dos títulos
\usepackage{listings}   % Para destacar código
\usepackage{xcolor}     % Para cores no código

% --- Configuração das Margens ---
\geometry{
    a4paper,
    left=3cm,
    top=3cm,
    right=2cm,
    bottom=2cm
}

% --- Configuração dos Tamanhos dos Títulos (NOVO) ---
% Reduzindo o tamanho padrão gigante do LaTeX para algo mais sóbrio
% Capítulo (Centralizado, Negrito, Tamanho LARGE)
\titleformat{\chapter}[block]
  {\normalfont\LARGE\bfseries\centering}{\thechapter}{1em}{}
\titlespacing*{\chapter}{0pt}{-30pt}{20pt} % Ajusta espaçamento antes/depois

% Seção (Alinhado a esquerda, Negrito, Tamanho Large)
\titleformat{\section}
  {\normalfont\Large\bfseries}{\thesection}{1em}{}

% Subseção (Alinhado a esquerda, Negrito, Tamanho normal plus)
\titleformat{\subsection}
  {\normalfont\large\bfseries}{\thesubsection}{1em}{}

% --- Configuração de Estilos de Código ---
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\begin{document}

% --------------------------------------------------------------------------
% CAPA (Atualizada com Logos)
% --------------------------------------------------------------------------
\begin{titlepage}
    \centering
    
    % --- Área dos Logos ---
    % IMPORTANTE: Faça upload de unicamp.png e feec.png para o seu projeto
    \begin{figure}[h]
        \centering
        \begin{minipage}{0.45\textwidth}
            \centering
            % Ajuste a largura (width) conforme necessário
            \includegraphics[width=3.5cm]{unicamp.png} 
        \end{minipage}
        \hfill
        \begin{minipage}{0.45\textwidth}
            \centering
            % Ajuste a largura (width) conforme necessário
            \includegraphics[width=3.5cm]{feec.png} 
        \end{minipage}
    \end{figure}
    % ----------------------
    
    \vspace*{1cm}
    
    {\large \textbf{UNICAMP -- UNIVERSIDADE ESTADUAL DE CAMPINAS}}\\
    {\large FACULDADE DE ENGENHARIA ELÉTRICA E DE COMPUTAÇÃO}
    
    \vspace{4cm}
    
    {\Large \textbf{ISAAC DO NASCIMENTO OLIVEIRA}}
    
    \vspace{4cm}
    
    {\Large \textbf{DESENVOLVIMENTO DE CAMADA DE SOFTWARE PARA ROBÔS UTILIZANDO ROS 2 E ORQUESTRAÇÃO VIA LANGCHAIN}}
    
    \vspace{4cm}
    
    \vfill
    
    {\large CAMPINAS}\\
    {\large 2026}
    
\end{titlepage}

% --------------------------------------------------------------------------
% FOLHA DE ROSTO
% --------------------------------------------------------------------------
\begin{titlepage}
    \centering
    \vspace*{1cm}
    
    {\large \textbf{ISAAC DO NASCIMENTO OLIVEIRA}}
    
    \vspace{4cm}
    
    {\Large \textbf{DESENVOLVIMENTO DE CAMADA DE SOFTWARE PARA ROBÔS UTILIZANDO ROS 2 E ORQUESTRAÇÃO VIA LANGCHAIN}}
    
    \vspace{4cm}
    
    \begin{flushright}
    \begin{minipage}{0.5\textwidth}
        Trabalho de Iniciação Científica apresentado como requisito para disciplina EE015, orientado pelo Dr. Josué Ramos (CTI Renato Archer).
    \end{minipage}
    \end{flushright}
    
    \vfill
    
    {\large CAMPINAS}\\
    {\large 2026}
\end{titlepage}

% --------------------------------------------------------------------------
% RESUMO
% --------------------------------------------------------------------------
% Usando \chapter* para não numerar, mas mantendo o estilo visual
\chapter*{Resumo}
\addcontentsline{toc}{chapter}{Resumo} % Adiciona manualmente ao sumário
\onehalfspacing

O presente trabalho aborda o desenvolvimento de uma arquitetura de software para robôs de serviço, visando superar a rigidez das máquinas de estados finitos tradicionais através da integração com Grandes Modelos de Linguagem (LLMs). Utilizando o middleware ROS 2 (distribuição Jazzy) e orquestração via LangChain, foi desenvolvido um sistema capaz de coordenar percepção multimodal e navegação autônoma. O projeto validou a integração de nós de síntese e reconhecimento de voz, detecção de objetos via YOLO e navegação (SLAM/Nav2) em ambiente simulado (Gazebo/Warehouse) utilizando o robô Tugbot. Os resultados demonstram a viabilidade de controlar tópicos do ROS 2 através de comandos em linguagem natural, permitindo uma interação mais fluida e semântica entre humanos e robôs.

\vspace{1cm}
\noindent \textbf{Palavras-chave:} Robótica de Serviço. ROS 2 Jazzy. LangChain. Text-to-speech. Speech-to-text. Nav2.

\newpage

% --------------------------------------------------------------------------
% SUMÁRIO
% --------------------------------------------------------------------------
\tableofcontents
\newpage

% --------------------------------------------------------------------------
% CAPÍTULOS TEXTUAIS
% --------------------------------------------------------------------------
\onehalfspacing % Espaçamento 1.5 para o corpo do texto

\chapter{Introdução}

A robótica de serviços contemporânea demanda sistemas capazes de operar em ambientes não estruturados e interagir naturalmente com seres humanos. Todavia, a robótica tradicional fundamenta-se predominantemente em máquinas de estados finitos e comportamentos pré-programados. Essa abordagem apresenta limitações críticas, como a rigidez diante de novos cenários, a falta de compreensão contextual e o crescimento exponencial da complexidade do código conforme a tarefa se expande, o que dificulta a escalabilidade e a acessibilidade para usuários não especialistas.

Nesse contexto, a integração de \textit{Large Language Models} (LLMs) surge como uma alternativa para dotar sistemas robóticos de raciocínio semântico e tomada de decisão flexível. Diferente da programação rígida, os LLMs permitem a inferência sobre situações complexas e a generalização para cenários não previstos durante o treinamento. A utilização dessas redes neurais possibilita que instruções em linguagem natural sejam convertidas em ações dinâmicas, eliminando a necessidade de modificações constantes no código-fonte para cada nova subtarefa.

\section{Agentes ReAct vs. Máquinas de Estado Finitos}

A arquitetura proposta neste trabalho fundamenta-se no paradigma de \textit{agentes ReAct} (Reasoning and Acting)\footnote{YAO, Shunyu et al. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv:2210.03629, 2022.}, em contraposição aos chatbots tradicionais baseados em máquinas de estados finitos. Enquanto sistemas convencionais seguem fluxos pré-determinados de \texttt{IF-THEN-ELSE}, onde cada estado possível deve ser explicitamente programado, os agentes ReAct operam através de um ciclo iterativo de raciocínio e ação.

O ciclo ReAct segue a estrutura:
\begin{enumerate}
    \item \textbf{Thought (Pensamento):} O LLM analisa a situação e decide qual ação tomar.
    \item \textbf{Action (Ação):} Executa uma ferramenta específica (ex: publicar em tópico ROS, buscar na web).
    \item \textbf{Observation (Observação):} Recebe o resultado da ação executada.
    \item \textbf{Repetição:} Retorna ao passo 1 até completar o objetivo.
\end{enumerate}

Esta abordagem confere ao sistema a capacidade de \textit{tool-calling}, permitindo que o agente acione dinamicamente funções externas conforme a necessidade. Por exemplo, ao receber o comando ``navegue até o meio do mapa e me diga o que vê'', o agente pode:
\begin{itemize}
    \item Chamar a ferramenta \texttt{navigate\_to\_goal(x=0.0, y=0.0)}
    \item Aguardar conclusão da navegação
    \item Chamar \texttt{get\_current\_view()} para capturar imagem
    \item Analisar a imagem multimodalmente
    \item Retornar descrição verbal ao usuário
\end{itemize}

Diferentemente de uma máquina de estados, onde seria necessário programar explicitamente cada combinação de comandos, o agente ReAct generaliza o comportamento através do raciocínio linguístico, reduzindo drasticamente a complexidade de manutenção do código.

Este projeto propõe o desenvolvimento de uma arquitetura de software modular que utiliza o middleware ROS 2 Jazzy (Robotic Operating System), consolidado como padrão industrial devido ao seu suporte robusto a sistemas distribuídos. A arquitetura foca na integração entre o controle síncrono do robô e o processamento assíncrono das chamadas de IA generativa. Para a orquestração do fluxo cognitivo, emprega-se o ecossistema LangChain, que facilita a implementação de agentes capazes de utilizar ferramentas externas (\textit{tool-calling}) e manter a persistência de estados via memória de conversação.

Para a validação experimental sem a necessidade imediata de hardware físico, utiliza-se o simulador \textbf{Gazebo Harmonic}. Este ambiente provê simulação física realista com motor de física ODE (Open Dynamics Engine), suporte a sensores complexos (LiDAR omnidirecional, câmeras RGB-D) e renderização gráfica via OGRE. A integração com o ROS 2 ocorre através da biblioteca \texttt{ros\_gz\_bridge}, que mapeia tópicos do simulador para o middleware robótico, permitindo que o código desenvolvido em ambiente virtual seja diretamente portável para hardware real.

Complementarmente, emprega-se o \textbf{RViz2} como ferramenta de visualização e \textit{debugging}. O RViz permite a inspeção em tempo real da árvore de transformadas (TF Tree), visualização de nuvens de pontos provenientes do LiDAR, trajetórias planejadas pelo Nav2 e frames capturados pelas câmeras. Esta ferramenta foi essencial para validar a correta configuração dos sensores e identificar inconsistências na cadeia cinemática do robô durante o desenvolvimento.

Tal infraestrutura garante a robustez na comunicação entre percepção e atuação, permitindo testes de interação humano-robô em cenários controlados antes do deploy em sistemas físicos.

\chapter{Objetivos}

\section{Objetivo Geral}
Projetar e implementar uma arquitetura de software para robôs de serviço que integre percepção multimodal (visão e áudio), raciocínio semântico baseado em LLMs e navegação autônoma via ROS 2. O sistema deve interpretar comandos em linguagem natural, navegar autonomamente e interagir através de interfaces de voz, demonstrando a viabilidade de substituir máquinas de estados finitos por agentes cognitivos.

\section{Objetivos Específicos}
\begin{itemize}
    \item Estruturar ambiente de desenvolvimento híbrido integrando Python (gerenciador \texttt{uv}) e ROS 2 Jazzy com Gazebo Harmonic.
    \item Desenvolver nós ROS 2 para interfaces de áudio: reconhecimento de fala (STT via Groq/Whisper) e síntese de voz (TTS via Kokoro).
    \item Implementar sistema de visão computacional com captura sob demanda e análise multimodal via Gemini 2.5 Flash.
    \item Construir agente cognitivo utilizando LangChain com mecanismo de \textit{tool-calling} e persistência de contexto via \texttt{InMemorySaver}.
    \item Validar a arquitetura através da integração com Nav2 e SLAM Toolbox para navegação autônoma controlada por linguagem natural.
\end{itemize}

\chapter{Metodologia}

O projeto adotou uma abordagem experimental e incremental, fundamentada no desenvolvimento de Provas de Conceito (POCs) para validação isolada de subsistemas antes da integração final. O ambiente de desenvolvimento foi configurado sobre o sistema operacional Ubuntu, compatível com o ROS 2 Jazzy Jalisco, utilizando o gerenciador de pacotes \texttt{uv} para garantir o isolamento e a reprodutibilidade das dependências Python da camada cognitiva.

\subsection{Ferramentas Utilizadas}
A arquitetura proposta foi viabilizada através da integração das seguintes tecnologias:
\begin{itemize}
    \item \textbf{Middleware:} ROS 2 Jazzy Jalisco, atuando como a camada de comunicação distribuída.
    \item \textbf{Orquestração Cognitiva:} Framework LangChain para gestão de agentes, \textit{chains} e memória.
    \item \textbf{Simulação:} Gazebo Harmonic para modelagem física e sensores do robô Tugbot (URDF/SDF).
    \item \textbf{Navegação e Mapeamento:} Nav2 Stack para planejamento de trajetórias e SLAM Toolbox para geração de mapas.
    \item \textbf{Modelos de IA:} Modelos pré-treinados integrados via API e localmente: Kokoro (TTS), Whisper/Groq (STT) e Gemini 2.5 Flash/Pro (Cognição/Visão).
\end{itemize}

\subsection{Provas de Conceito (POCs)}
Para mitigar riscos de integração, o desenvolvimento foi segmentado em quatro módulos validados independentemente:

\begin{enumerate}
    \item \textbf{Interação por Voz:} Validação do pipeline STT/TTS com Kokoro e Groq Whisper, confirmando captura de áudio, transcrição e síntese de fala funcionais.
    \item \textbf{Percepção Visual:} Teste inicial com YOLOv8n para detecção de objetos, posteriormente substituído por análise multimodal com Gemini para maior flexibilidade semântica.
    \item \textbf{Agente Cognitivo:} Validação do ciclo de inferência LLM com ferramentas mockadas, testando o padrão ReAct e persistência de contexto via MemorySaver.
    \item \textbf{Navegação:} Configuração da TF Tree, bridge Gazebo-ROS 2, validação do SLAM Toolbox e testes de controle teleoperado.
\end{enumerate}

A validação final ocorreu em um cenário de galpão (\textit{Warehouse}), onde se testou a comunicação ponta a ponta entre a percepção sensorial (áudio e visão), o raciocínio do agente e a execução motora no simulador. Esta etapa confirmou a robustez da \textit{bridge} entre os processos assíncronos da IA e os requisitos síncronos de controle do robô.

\chapter{Desenvolvimento}

\section{Configuração do Ambiente e Arquitetura}
A estrutura inicial do projeto foi estabelecida de forma modular, segregando as modalidades de interação do agente (texto, fala, escuta e visão). A arquitetura centraliza-se em um módulo cognitivo, que processa as chamadas de IA Generativa via framework LangChain. 

Diferentemente da abordagem tradicional de pacotes ROS 2 (que exigem arquivos \texttt{package.xml}, \texttt{setup.py} e processo de \textit{build} via \texttt{colcon}), optou-se por scripts Python independentes que importam as bibliotecas ROS diretamente. Esta decisão arquitetural simplificou o desenvolvimento iterativo, permitindo testes rápidos sem recompilação. A seguir, um exemplo da configuração do nó cognitivo:

\begin{lstlisting}[language=Python, caption={Configuração do Nó Cognitivo (\texttt{think.py})}]
# src/senses/think.py
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
import threading
from langchain.agents import create_agent
from langgraph.checkpoint.memory import InMemorySaver
from langchain_google_genai import ChatGoogleGenerativeAI
from src.graph.tools import all_tools

SYSTEM_PROMPT = """
Voce e uma robo ROS2 inteligente chamada Sandra.
Use as ferramentas disponiveis para ajuda-la a responder.
Responda de forma concisa e direta.
"""

class Think_Node(Node):
    def __init__(self):
        super().__init__('think_node')
        self.sub_stt = self.create_subscription(
            String, '/transcript', self.callback_stt, 10)
        self.pub_tts = self.create_publisher(
            String, '/tts_command', 10)
        
        # Inicializacao do agente LLM
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash")
        self.checkpointer = InMemorySaver()
        self.agent = create_agent(
            self.llm, all_tools, 
            system_prompt=SYSTEM_PROMPT,
            checkpointer=self.checkpointer)
    
    def callback_stt(self, msg: String):
        threading.Thread(
            target=self.process_input, 
            args=(msg.data,), daemon=True).start()
\end{lstlisting}

Esta configuração permite que o nó \texttt{think.py} atue simultaneamente como subscriber do tópico \texttt{/transcript} (entrada de comandos) e publisher em \texttt{/tts\_command} (saída de respostas), enquanto mantém o ciclo de inferência do LLM isolado em threads para evitar bloqueios na comunicação ROS.

\section{Interfaces de Áudio (Sentidos)}
Foram desenvolvidos nós específicos para interação verbal:
\begin{itemize}
    \item \textbf{Fala (TTS):} Integração do modelo Kokoro, otimizado para rodar localmente com baixa latência, publicando áudio via biblioteca \texttt{sounddevice}.
    \item \textbf{Escuta (STT):} Implementação de captura de áudio e transcrição, permitindo que o robô receba comandos de voz e os converta em texto para a LLM.
    \item \textbf{Texto:} Entrada via teclado para recepção de comandos pela LLM, visando a otimização do processo de \textit{debugging} e servindo como alternativa à interface de voz.
\end{itemize}

\section{Camada Cognitiva e Visão}
O ``cérebro'' do robô foi implementado utilizando LangChain. O sistema foi capacitado para:
\begin{enumerate}
    \item Interpretar a intenção do usuário via LLM (ex: modelos locais ou via API).
    \item Acionar ferramentas (\textit{Tools}) correspondentes, como ``Navegar para uma posição'' ou ``Publicar em tópico''.
    \item \textbf{Percepção Visual:} Integração de input multimodal para processar imagens vindas dos tópicos da câmera simulada, permitindo que a LLM tenha contexto sobre objetos no ambiente.
\end{enumerate}

\subsection{Ferramentas ROS 2 Inspiradas no Projeto NASA ROSA}

O desenvolvimento das ferramentas de interação com o middleware ROS 2 foi inspirado no projeto \textit{Robot Operating System Agent} (ROSA) da NASA\footnote{NASA. ROSA - Robot Operating System Agent. Disponível em: \url{https://github.com/nasa/ROSA}}. Duas ferramentas fundamentais foram adaptadas:

\subsubsection{Ferramenta de Publicação em Tópicos}
\begin{lstlisting}[language=Python, caption={Ferramenta de Publicação em Tópicos ROS 2}]
# src/graph/tools/ros2.py
from langchain_core.tools import tool
import subprocess

@tool
def publish_to_topic(topic: str, msg_type: str, 
                     data: str) -> str:
    """
    Publica mensagem em um topico ROS 2.
    Args:
        topic: Nome do topico (ex: /cmd_vel)
        msg_type: Tipo da mensagem 
                  (ex: geometry_msgs/msg/Twist)
        data: Dados no formato YAML 
              (ex: '{linear: {x: 0.5}}')
    """
    cmd = f"ros2 topic pub --once {topic} {msg_type} \
           \"{data}\""
    result = subprocess.run(
        cmd, shell=True, capture_output=True, text=True)
    
    if result.returncode == 0:
        return f"Publicado em {topic} com sucesso"
    else:
        return f"Erro: {result.stderr}"
\end{lstlisting}

\subsubsection{Ferramenta de Execução de Comandos ROS}
\begin{lstlisting}[language=Python, caption={Ferramenta de Execução de Comandos ROS 2}]
@tool
def execute_ros_command(command: str) -> dict:
    """
    Executa comandos ROS 2 validados.
    Permite: ros2 node, topic, service, param, doctor
    """
    allowed_prefixes = [
        'ros2 node', 'ros2 topic', 'ros2 service',
        'ros2 param', 'ros2 doctor'
    ]
    
    if not any(command.startswith(p) 
               for p in allowed_prefixes):
        return {
            "success": False,
            "message": "Comando nao autorizado"
        }
    
    result = subprocess.run(
        command, shell=True, capture_output=True, 
        text=True, timeout=10
    )
    
    return {
        "success": result.returncode == 0,
        "output": result.stdout,
        "error": result.stderr
    }
\end{lstlisting}

\subsection{Ferramenta de Busca na Web e Visão}

Além das ferramentas ROS, o agente possui acesso a recursos externos:

\begin{itemize}
    \item \textbf{Busca na Web:} Integração com a API Serper (Google Search) permite que o robô consulte informações externas quando necessário. Por exemplo, ao questionar ``qual a temperatura ideal para armazenar medicamentos?'', o agente pode buscar a resposta online antes de responder ao usuário.

\begin{lstlisting}[language=Python, caption={Ferramenta de Busca Web via Serper API (\texttt{external.py})}, label=lst:web_search]
from langchain_community.utilities import GoogleSerperAPIWrapper
from langchain_core.tools import tool

search_wrapper = GoogleSerperAPIWrapper()

@tool("web_search")
def google_serper_search(query: str) -> str:
    """Realiza uma pesquisa na internet usando Serper 
    e retorna informacoes relevantes resumidas."""
    try:
        result = search_wrapper.run(query)
        return result or "Nenhum resultado encontrado."
    except Exception as e:
        return f"Erro ao pesquisar: {e}"
\end{lstlisting}
    
    \item \textbf{Visão Computacional:} A ferramenta \texttt{get\_current\_view()} implementa um sistema de captura sob demanda. Quando invocada, cria um arquivo de sinalização (\texttt{/tmp/vision\_request}), aguarda o nó \texttt{vision.py} capturar o frame da câmera, lê a imagem salva, converte para base64 e retorna ao LLM. Este mecanismo evita o processamento contínuo de frames, otimizando recursos computacionais. O modelo multimodal então analisa a imagem e gera descrições semânticas do ambiente.
\end{itemize}

\section{Navegação e Simulação (Nav2)}
A etapa final focou na integração com o stack de navegação. O robô Tugbot foi inserido no ambiente \textit{Warehouse} do Gazebo Harmonic.

\subsection{Ambiente de Simulação - Gazebo Harmonic}
O Gazebo Harmonic é um simulador físico de código aberto que provê:
\begin{itemize}
    \item \textbf{Motor de Física:} ODE (Open Dynamics Engine) para simulação de colisões, fricção e dinâmica de corpos rígidos.
    \item \textbf{Sensores Virtuais:} LiDAR omnidirecional (360°), câmeras RGB e de profundidade, sensores de contato e IMU.
    \item \textbf{Renderização:} OGRE (Object-Oriented Graphics Rendering Engine) para visualização 3D realista.
    \item \textbf{Bridge ROS 2:} Plugin \texttt{ros\_gz\_bridge} que mapeia tópicos do simulador (\texttt{gz.msgs}) para mensagens ROS (\texttt{sensor\_msgs}, \texttt{geometry\_msgs}).
\end{itemize}

O ambiente \textit{Warehouse} simula um galpão industrial com prateleiras, obstáculos e iluminação configurável, permitindo validar algoritmos de navegação em cenários complexos.

\subsection{Visualização e Debugging - RViz2}
O RViz2 foi fundamental para validação da arquitetura sensorial:
\begin{itemize}
    \item \textbf{TF Tree Viewer:} Validação da árvore de transformadas geométricas, essencial para garantir que os sensores estejam corretamente posicionados em relação ao corpo do robô (\texttt{base\_link}).
    \item \textbf{LaserScan Display:} Visualização em tempo real da nuvem de pontos do LiDAR, permitindo identificar problemas de oclusão ou configuração incorreta do sensor.
    \item \textbf{Map Display:} Sobreposição do mapa gerado pelo SLAM com as leituras sensoriais atuais, facilitando o \textit{debugging} de problemas de localização.
    \item \textbf{Path Planning:} Exibição das trajetórias planejadas e executadas pelo Nav2, com indicadores visuais de \textit{costmaps} (global e local).
\end{itemize}

\subsection{Configuração do Stack de Navegação}
\begin{itemize}
    \item \textbf{SLAM:} Configuração do SLAM Toolbox para mapeamento do ambiente desconhecido, utilizando o algoritmo Karto para otimização de grafo de poses.
    \item \textbf{Nav2:} Configuração dos \textit{costmaps} (global e local) e planejadores de trajetória (NavFn para planejamento global, DWB para controle local) permitindo a navegação autônoma com desvio dinâmico de obstáculos.
\end{itemize}

A Figura \ref{fig:architecture} apresenta o diagrama completo da arquitetura, ilustrando os dois cenários principais de operação: percepção visual multimodal (Cenário 1) e navegação via comando semântico (Cenário 2). O diagrama evidencia o fluxo de dados entre os nós ROS 2, destacando os tópicos de comunicação, arquivos de sinalização e a integração com sistemas externos (Gazebo, Nav2, Gemini).

\begin{figure}[H]
    \centering
    % CERTIFIQUE-SE QUE A IMAGEM arquitetura_ros2_detalhada.png ESTÁ NO PROJETO
    \includegraphics[width=0.98\textwidth]{arquitetura_ros2_detalhada.png}
    \caption{Arquitetura completa do sistema mostrando os dois fluxos principais: (Esquerda) Cenário 1 - Percepção visual com LLM multimodal; (Direita) Cenário 2 - Navegação autônoma via comando semântico. As cores distinguem os tipos de componentes: verde (sensores), azul (percepção), dourado (cognição), salmão (ferramentas), roxo (atuação), cinza (tópicos/arquivos) e rosa (sistemas externos).}
    \label{fig:architecture}
    \small{Fonte: O autor (Gerado via script \texttt{visualize\_ros\_graph.py}).}
\end{figure}


\section{Fluxos de Execução da Arquitetura}

Para validar a integração entre os módulos, foram estabelecidos fluxos cognitivos que exemplificam a interação entre o processamento de linguagem natural e a atuação robótica.

\subsection{Cenário 1: Percepção Visual Multimodal}
Este fluxo descreve a solicitação do usuário para que o robô descreva o ambiente (\textit{"Sandra, me mostre o que você vê"}):

\begin{enumerate}
    \item \textbf{Percepção:} O nó \texttt{talk.py} realiza o \textit{streaming} de áudio, converte-o em texto e publica a \textit{string} no tópico \texttt{/transcript}.
    \item \textbf{Cognição (Inferência):} O nó \texttt{think.py} recebe o texto e invoca o agente LLM via LangChain. O modelo identifica a necessidade de visão e aciona a ferramenta (\textit{tool}) \texttt{get\_current\_view}.
    \item \textbf{Execução de Ferramenta:} A ferramenta captura a imagem mais recente do tópico de visão (processada pelo \texttt{vision.py}), converte-a para base64 e a envia de volta ao modelo multimodal.
    \item \textbf{Análise e Resposta:} O LLM analisa a imagem, gera uma descrição semântica e a envia para o tópico \texttt{/tts\_command}.
    \item \textbf{Atuação:} O nó \texttt{speech.py} recebe o comando, realiza a síntese de voz via Kokoro e executa o áudio pelo \textit{hardware} de saída.
\end{enumerate}

\subsection{Cenário 2: Navegação via Comando Semântico}
Este fluxo ilustra a conversão de uma intenção do usuário em uma coordenada geográfica no mapa (\textit{"Vá até o meio do mapa"}):

\begin{enumerate}
    \item \textbf{Processamento de Intenção:} O agente LLM recebe o comando e reconhece que se trata de uma tarefa de deslocamento.
    \item \textbf{Mapeamento de Coordenadas:} O agente aciona a ferramenta de navegação, mapeando o conceito semântico "meio do mapa" para coordenadas cartesianas $(x, y)$ pré-definidas ou calculadas.
    \item \textbf{Comunicação com o Nav2:} O nó de controle publica o objetivo (\textit{Goal}) no \textit{Action Server} do \texttt{Nav2}.
    \item \textbf{Execução Motora:} O robô inicia o planejamento de trajetória e o movimento no simulador \textit{Gazebo}, enquanto o agente confirma verbalmente o início da tarefa.
\end{enumerate}


\chapter{Resultados}

Os experimentos realizados validaram a viabilidade da arquitetura proposta, integrando percepção multimodal, raciocínio baseado em LLMs e middleware via ROS 2. A estrutura modular permitiu a operação de cinco nós independentes, com o agente cognitivo gerenciando automaticamente suas ferramentas.

\section{Controle via Linguagem Natural}

O agente demonstrou capacidade de interpretar comandos textuais imprecisos e convertê-los em ações concretas. Através do mecanismo de \textit{tool-calling}, comandos como ``faça o robô girar'' resultam em trajetórias circulares sem necessidade de código pré-programado para essa variação específica. O sistema generalizou corretamente variações de comandos não explicitamente programadas, como ``rode no sentido horário'' ou ``dê uma volta completa''.

\section{Integração Sensorial e Visão Multimodal}

O sistema integra uma câmera RGB-D simulada, permitindo que o agente descreva semanticamente o ambiente. A implementação de captura sob demanda reduziu significativamente o uso de CPU durante períodos ociosos. O modelo multimodal Gemini 2.5 Flash processa imagens e gera descrições contextualizadas do ambiente visualizado.

\textbf{Limitações identificadas:}
\begin{itemize}
    \item A resolução de captura (1280x720) impacta a detecção de detalhes finos.
    \item O modelo ocasionalmente apresenta alucinações visuais em condições de iluminação inadequada.
\end{itemize}

\subsection{Interface de Voz}

A interface utiliza Kokoro (TTS) e Groq Whisper (STT). Foram identificados desafios na interpretação de termos bilíngues (ex: ``ROS dois'' transcrito incorretamente) e fonética de siglas (``Nav2'' pronunciado incorretamente). Implementou-se dicionário de substituição contextual e espaçamento de siglas como soluções paliativas.

\section{Navegação Autônoma e SLAM}

O SLAM Toolbox gerou mapas coerentes do ambiente \textit{Warehouse}, e o Nav2 executou trajetórias de ponto a ponto com sucesso. O sistema demonstrou capacidade de desvio de obstáculos e planejamento de trajetórias otimizadas.

\noindent \textbf{Vídeo de Demonstração:} [INSERIR LINK AQUI] (O vídeo apresenta o robô recebendo comandos de voz, processando a semântica da instrução e executando navegação autônoma dentro do simulador Gazebo).

\chapter{Conclusão}

O presente projeto demonstrou com sucesso o desenvolvimento de uma arquitetura funcional que integra a robustez do middleware ROS 2 à flexibilidade dos \textit{Large Language Models} (LLMs). A substituição da lógica de controle tradicional por orquestração cognitiva via LangChain permitiu atingir um nível superior de abstração, desacoplando a tomada de decisão da execução motora.

\section{Principais Contribuições}

\begin{enumerate}
    \item \textbf{Sistema de Visão Sob Demanda:} Mecanismo de sinalização por arquivo (\texttt{/tmp/vision\_request}) que reduz significativamente o uso de CPU durante períodos ociosos, aplicável a outros subsistemas que exijam processamento intensivo sob demanda.
    
    \item \textbf{Padrão de Integração Multimodal:} Solução para processamento de imagens em agentes LLM (resposta de ferramenta + mensagem de usuário com imagem), estabelecendo padrão reproduzível para projetos de robótica multimodal.
    
    \item \textbf{Arquitetura Modular:} Implementação de nós ROS 2 como scripts Python standalone reduz tempo de iteração de desenvolvimento, vantajoso para prototipagem rápida.
    
    \item \textbf{Ferramentas ROS Seguras:} Validação de lista branca para execução de comandos (\texttt{execute\_ros\_command}), prevenindo injeção de comandos maliciosos.
\end{enumerate}

\section{Impacto e Democratização}

A arquitetura proposta permite incorporar novos comportamentos sem reprogramação estrutural, adaptar o sistema a diferentes domínios via modificação de prompts, escalar para sistemas multi-agente e tornar a tecnologia acessível a não-programadores através de interfaces em linguagem natural.

\section{Limitações e Trabalhos Futuros}

\textbf{Limitações identificadas:}
\begin{itemize}
    \item Latência de inferência pode ser problemática em cenários críticos que exijam resposta instantânea.
    \item Custo computacional moderado limita deploy em hardware embarcado de baixa potência.
    \item Ausência de mecanismos de verificação de segurança antes de ações potencialmente perigosas.
\end{itemize}

\textbf{Trabalhos futuros:}
\begin{itemize}
    \item Transição para hardware físico (TurtleBot 4, robôs industriais).
    \item Otimização computacional via quantização de modelos e cache de respostas.
    \item Expansão de capacidades multimodais (sensores táteis, processamento 3D).
    \item Validação em cenários reais (hospitais, logística, residências assistidas).
\end{itemize}

\section{Considerações Finais}

Este trabalho estabelece um paradigma de controle robótico que alia confiabilidade de sistemas distribuídos à capacidade de raciocínio abstrato de modelos de linguagem. A arquitetura modular e extensível demonstra maturidade para aplicações práticas. A democratização da robótica através de interfaces naturais representa um passo crucial para adoção massiva de robôs de serviço em ambientes não industriais.

O código-fonte completo, documentação técnica e vídeos de demonstração encontram-se disponíveis no repositório do projeto, permitindo reprodução e extensão por outros pesquisadores.

% --------------------------------------------------------------------------
% REFERÊNCIAS
% --------------------------------------------------------------------------
% Mudando o estilo do título das referências para combinar com o resto
\renewcommand{\bibname}{Referências Bibliográficas}

\begin{thebibliography}{9}
% Adiciona ao sumário
\addcontentsline{toc}{chapter}{Referências Bibliográficas}

\bibitem{g1_2025}
CENTRO de tecnologia quer unir IA brasileira para criar robôs mais humanos e empáticos. \textbf{G1}, Campinas e Região, 24 ago. 2025. Disponível em: \url{https://g1.globo.com/sp/campinas-regiao/noticia/2025/08/24/centro-de-tecnologia-quer-unir-ia-brasileira-para-criar-robos-mais-humanos-e-empaticos.ghtml}. Acesso em: 15 jan. 2026.

\bibitem{langchain_docs}
LANGCHAIN. \textbf{LangChain Python Documentation}. Documentação oficial da biblioteca de orquestração de LLMs. Disponível em: \url{https://python.langchain.com/}. Acesso em: 15 jan. 2026.

\bibitem{ros2_docs}
OPEN ROBOTICS. \textbf{ROS 2 Documentation: Jazzy Jalisco}. Documentação oficial do sistema operacional de robôs. Disponível em: \url{https://docs.ros.org/en/jazzy/}. Acesso em: 15 jan. 2026.

\bibitem{nav2_paper}
MACENSKY, S. et al. The Navigation2 Stack: Navigating in ROS 2. In: \textbf{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}. 2020.

\end{thebibliography}

\end{document}