import os
import uuid
from typing import Annotated, Literal
from typing_extensions import TypedDict

# --- Depend√™ncias do LangChain ---
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.tools import tool
from langchain_core.messages import AnyMessage, HumanMessage, ToolMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import ToolNode

# --- CONFIGURA√á√ÉO DA API KEY ---
from dotenv import load_dotenv
load_dotenv(override=True)

# =============================================================================
# 1. FERRAMENTAS MOCKADAS (Simulando o Rob√¥)
# =============================================================================

@tool
def tool_ros_speak(text: str) -> str:
    """
    Simula o rob√¥ falando. Use esta ferramenta para dar a resposta final ao usu√°rio.
    :param text: O texto que o rob√¥ deve falar.
    """
    print(f"\nü§ñ [ROB√î FALA]: {text}\n")
    return "O rob√¥ terminou de falar."

@tool
def tool_ros_navigate(destination: str) -> str:
    """
    Simula o rob√¥ navegando para um destino (ex: 'sala 5', 'cozinha').
    :param destination: O nome do local para onde navegar.
    """
    print(f"\nü§ñ [ROB√î NAVEGA]: Iniciando navega√ß√£o para {destination}...")
    return f"Navega√ß√£o para {destination} iniciada com sucesso."

@tool
def tool_describe_scene() -> str:
    """
    Simula o VLM (Vision Language Model) descrevendo a cena atual.
    Use isso se o usu√°rio perguntar o que o rob√¥ est√° vendo.
    """
    print(f"\nü§ñ [VIS√ÉO VLM]: Processando a cena atual...")
    # Resposta mockada de um VLM
    return "Eu estou vendo uma sala de estar com um sof√° e uma pessoa sentada."

@tool
def tool_call_rag(query: str) -> str:
    """
    Simula uma busca no RAG (Base de Conhecimento) para responder perguntas
    factuais (ex: 'onde fica a sala 5?').
    :param query: A pergunta para a base de conhecimento.
    """
    print(f"\nüß† [RAG]: Buscando por '{query}'...")
    knowledge_db = {
        "onde fica a sala 5": "A Sala 5 fica no segundo andar, ao lado da TI.",
        "qual o evento de hoje": "O 'Tech Demo Day' come√ßa √†s 15h no audit√≥rio."
    }
    
    response = "Desculpe, n√£o encontrei essa informa√ß√£o."
    for key in knowledge_db:
        if key in query.lower():
            response = knowledge_db[key]
            break
    return response

# =============================================================================
# 2. DEFINI√á√ÉO DO GRAFO (O C√©rebro)
# =============================================================================

# --- O Estado da Mem√≥ria ---
class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], "add_messages"]

# --- O PROMPT DE SISTEMA (A CORRE√á√ÉO CR√çTICA) ---
# Este prompt FOR√áA o LLM a n√£o entrar em loop e a usar a ferramenta de fala.
system_prompt = """Voc√™ √© o c√©rebro de um rob√¥.
Seu objetivo √© responder ao usu√°rio. Voc√™ tem 4 ferramentas:

1.  `tool_call_rag`: Para buscar informa√ß√µes factuais (ex: "Onde fica...").
2.  `tool_describe_scene`: Para descrever o que voc√™ v√™ (ex: "O que voc√™ v√™?").
3.  `tool_ros_navigate`: Para se mover (ex: "V√° para...").
4.  `tool_ros_speak`: Para falar com o usu√°rio.

**REGRA MAIS IMPORTANTE:**
Ap√≥s usar uma ferramenta de coleta de dados (RAG, VLM, Navega√ß√£o), voc√™ DEVE usar a 
ferramenta `tool_ros_speak` para comunicar o resultado ao usu√°rio. 
N√ÉO chame a mesma ferramenta de dados duas vezes.

Fluxo de exemplo:
Usu√°rio: "O que voc√™ v√™?"
Voc√™: (Chama `tool_describe_scene`)
Tool: "Vejo um sof√°."
Voc√™: (Chama `tool_ros_speak` com o texto "Eu estou vendo um sof√°.")
"""

# --- Configura√ß√£o do LLM e Ferramentas ---

# CORRE√á√ÉO: Passe o system_prompt DIRETAMENTE para o modelo aqui
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash", # Ou o modelo Gemini que voc√™ estiver usando
)

tools = [tool_ros_speak, tool_ros_navigate, tool_describe_scene, tool_call_rag]
tool_node = ToolNode(tools) # N√≥ que executa as ferramentas


# --- O C√©rebro (Prompt + LLM + Ferramentas) ---

# CORRE√á√ÉO: Simplifique o template do prompt.
# Ele s√≥ precisa lidar com o hist√≥rico de mensagens.
prompt = ChatPromptTemplate.from_messages(
    [
        MessagesPlaceholder(variable_name="messages"), # <-- S√ì ISSO
    ]
)

llm_with_tools = prompt | llm.bind_tools(tools)


# --- N√ìS DO GRAFO ---

def llm_router_node(state: AgentState):
    """O n√≥ principal que chama o LLM para decidir o que fazer."""
    print("--- \nüß† [C√âREBRO]: Pensando...")
    response = llm_with_tools.invoke(state) # Passa o estado (com prompt)
    return {"messages": [response]}

def should_continue(state: AgentState) -> Literal["call_tool", "__end__"]:
    """Decide se o grafo deve chamar uma ferramenta ou se a conversa terminou."""
    last_message = state["messages"][-1]
    
    if isinstance(last_message, AIMessage) and last_message.tool_calls:
        print(f"üß† [C√âREBRO]: Decidi usar: {[tc['name'] for tc in last_message.tool_calls]}")
        return "call_tool"
    
    # Se o LLM n√£o chamou uma ferramenta, o prompt de sistema falhou
    # ou √© uma conversa simples que n√£o precisa de 'tool_ros_speak'.
    print("üß† [C√âREBRO]: Decidi encerrar o turno (resposta final).")
    return "__end__"


# --- CONSTRU√á√ÉO DO GRAFO ---

def build_graph():
    workflow = StateGraph(AgentState)
    
    workflow.add_node("llm_router", llm_router_node)
    workflow.add_node("call_tool", tool_node)

    workflow.set_entry_point("llm_router")

    workflow.add_conditional_edges(
        "llm_router",
        should_continue,
        {
            "call_tool": "call_tool",
            "__end__": END
        }
    )
    workflow.add_edge("call_tool", "llm_router")

    memory = MemorySaver()
    print("‚úÖ C√©rebro (Grafo) compilado com sucesso!")
    return workflow.compile(checkpointer=memory)

# =============================================================================
# 3. SCRIPT DE TESTE (Execu√ß√£o)
# =============================================================================

if __name__ == "__main__":
    app = build_graph()
    
    convo_id = str(uuid.uuid4())
    config = {"configurable": {"thread_id": convo_id}}

    print("\n=======================================================")
    print("C√©rebro do Rob√¥ Mockado (Vers√£o Est√°vel)")
    print("Fale com o rob√¥. Digite 'sair' para fechar.")
    print("=======================================================")

    while True:
        user_input = input("üôÇ [VOC√ä]: ")
        if user_input.lower() == "sair":
            print("ü§ñ [ROB√î]: Desligando...")
            break
        
        # Envia a mensagem humana para o grafo
        events = app.stream(
            {"messages": [HumanMessage(content=user_input)]},
            config,
            stream_mode="values"
        )
        
        # O stream executa o loop (LLM -> Tool -> LLM -> ...)
        # As pr√≥prias ferramentas j√° imprimem suas a√ß√µes
        for event in events:
            # Apenas monitoramos o evento final
            last_message = event["messages"][-1]
            if isinstance(last_message, AIMessage) and not last_message.tool_calls:
                # Se o LLM respondeu sem chamar o 'speak' (violando a regra)
                if last_message.content:
                     print(f"ü§ñ [ROB√î FALA - fallback]: {last_message.content}")
                pass